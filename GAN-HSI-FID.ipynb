{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  999\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import utils\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "import pathlib\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import spectral.io.envi as envi\n",
    "from spectral import imshow\n",
    "import math\n",
    "import csv\n",
    "\n",
    "manualSeed = 999\n",
    "debug = True\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "file_tracker = {\n",
    "    1: {\"counter\": 0, \"path\": 'Preliminary measurements on breast tissue/February 2025/270225/Paziente1/Paziente1M1/Paziente1M1_nrm.hdr'},\n",
    "    2: {\"counter\": 0, \"path\": 'Preliminary measurements on breast tissue/February 2025/270225/Paziente1/Paziente1M2/Paziente1M2_nrm.hdr'},\n",
    "    3: {\"counter\": 0, \"path\": 'Preliminary measurements on breast tissue/February 2025/270225/Paziente2/Paziente2M1_nrm.hdr'},\n",
    "    4: {\"counter\": 0, \"path\": 'Preliminary measurements on breast tissue/February 2025/270225/Paziente3/Paziente3M1/Paziente3M1_nrm.hdr'},\n",
    "    5: {\"counter\": 0, \"path\": 'Preliminary measurements on breast tissue/February 2025/270225/Paziente3/Paziente3M2/Paziente3M2_nrm.hdr'},\n",
    "    6: {\"counter\": 0, \"path\": 'Preliminary measurements on breast tissue/February 2025/270225/Paziente3/Paziente3M3/Paziente3M3_nrm.hdr'},\n",
    "    7: {\"counter\": 0, \"path\": 'Preliminary measurements on breast tissue/July 2024/170724/Paziente1/Sample1_2_M1_nrm.hdr'},\n",
    "    8: {\"counter\": 0, \"path\": 'Preliminary measurements on breast tissue/July 2024/170724/Paziente1/Sample1_2_M2_nrm.hdr'},\n",
    "    9: {\"counter\": 0, \"path\": 'Preliminary measurements on breast tissue/July 2024/170724/Paziente2/Set1/Sample_2_1_M1_nrm.hdr'},\n",
    "    10: {\"counter\": 0, \"path\": 'Preliminary measurements on breast tissue/July 2024/170724/Paziente2/Set1/Sample_2_1_M2_nrm.hdr'},\n",
    "    11: {\"counter\": 0, \"path\": 'Preliminary measurements on breast tissue/July 2024/170724/Paziente2/Set2/Sample_2_2_M1_nrm.hdr'},\n",
    "    12: {\"counter\": 0, \"path\": 'Preliminary measurements on breast tissue/July 2024/170724/Paziente2/Set2/Sample_2_2_M2_nrm.hdr'},\n",
    "    13: {\"counter\": 0, \"path\": 'Preliminary measurements on breast tissue/July 2024/180724/Paziente3/Sample3_M1_nrm.hdr'},\n",
    "    14: {\"counter\": 0, \"path\": 'Preliminary measurements on breast tissue/July 2024/180724/Paziente3/Sample3_M2_nrm.hdr'},\n",
    "    15: {\"counter\": 0, \"path\": 'Preliminary measurements on breast tissue/July 2024/180724/Paziente4/Set1/Sample4_latoA_M1_nrm.hdr'},\n",
    "    16: {\"counter\": 0, \"path\": 'Preliminary measurements on breast tissue/July 2024/180724/Paziente4/Set1/Sample4_latoA_M2_nrm.hdr'},\n",
    "    17: {\"counter\": 0, \"path\": 'Preliminary measurements on breast tissue/July 2024/180724/Paziente4/Set2/Sample4_latoB_M1_nrm.hdr'},\n",
    "    18: {\"counter\": 0, \"path\": 'Preliminary measurements on breast tissue/July 2024/180724/Paziente4/Set2/Sample4_latoB_M2_nrm.hdr'},\n",
    "    19: {\"counter\": 0, \"path\": 'Preliminary measurements on breast tissue/November 2024/141124/141124/Paziente1/Paziente1M1/Paziente1M1_nrm.hdr'},\n",
    "    20: {\"counter\": 0, \"path\": 'Preliminary measurements on breast tissue/November 2024/141124/141124/Paziente1/Paziente1M2/Paziente1M2_nrm.hdr'},\n",
    "    21: {\"counter\": 0, \"path\": 'Preliminary measurements on breast tissue/November 2024/141124/141124/Paziente1/Paziente1M3/Paziente1M3_nrm.hdr'},\n",
    "    22: {\"counter\": 0, \"path\": 'Preliminary measurements on breast tissue/November 2024/141124/141124/Paziente2/Paziente2M1/Paziente2M1_nrm.hdr'},\n",
    "    23: {\"counter\": 0, \"path\": 'Preliminary measurements on breast tissue/November 2024/141124/141124/Paziente2/Paziente2M2/Paziente2M2_nrm.hdr'},\n",
    "    24: {\"counter\": 0, \"path\": 'Preliminary measurements on breast tissue/November 2024/141124/141124/Paziente2/Paziente2M3/Paziente2M3_nrm.hdr'},\n",
    "    25: {\"counter\": 0, \"path\": 'Preliminary measurements on breast tissue/November 2024/141124/141124/Paziente3/Paziente3M1/Paziente3M1_nrm.hdr'},\n",
    "    26: {\"counter\": 0, \"path\": 'Preliminary measurements on breast tissue/November 2024/141124/141124/Paziente3/Paziente3M2/Paziente3M2_nrm.hdr'}   \n",
    "}\n",
    "\n",
    "SAMPLES_PER_IMAGE = 60\n",
    "IMG_SIZE = 244\n",
    "STD_THRESHOLD = 0.15\n",
    "X_BOUND = 1024\n",
    "Y_BOUND = 1280\n",
    "CHANNELS = 3\n",
    "Z_SIZE_REDUCTION = 100\n",
    "Z_SIZE = 128\n",
    "RGB_BANDS = [29, 17, 7]\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_img_samples(file_path, samples_per_img, img_size, x_bound, y_bound, std_threshold, debug=False):\n",
    "    img = envi.open(file_path)\n",
    "    data_array = img.load()\n",
    "    data_array = np.nan_to_num(data_array)\n",
    "    sample_list = []\n",
    "    coordinates_list = []    \n",
    "    while len(sample_list) < samples_per_img:\n",
    "        lower_x_bound = random.randint(0, x_bound - img_size - 1)\n",
    "        lower_y_bound = random.randint(0, y_bound - img_size - 1)\n",
    "        sample_array = data_array[lower_x_bound: lower_x_bound + img_size, lower_y_bound:lower_y_bound + img_size, :]\n",
    "        spatial_std_per_band = np.std(sample_array, axis=(2))\n",
    "        if (np.mean(spatial_std_per_band) > std_threshold):\n",
    "            sample_array = get_pca_sample_array(sample_array)\n",
    "            sample_list.append(sample_array)\n",
    "            if (debug):\n",
    "                coordinates_list.append((lower_x_bound,lower_y_bound))             \n",
    "    return sample_list\n",
    "\n",
    "def get_pca_sample_array(sample_array):\n",
    "    scaler = MinMaxScaler()   \n",
    "\n",
    "    sample_array = sample_array[:, :, [RGB_BANDS]].squeeze()\n",
    "    height, width, bands = sample_array.shape \n",
    "    sample_array = np.transpose(sample_array, (2, 0, 1))\n",
    "    return sample_array\n",
    "\n",
    "def samples_dict_init():\n",
    "    return {\n",
    "    1: {\"counter\": 0, \"sample_list\": []},\n",
    "    2: {\"counter\": 0, \"sample_list\": []},\n",
    "    3: {\"counter\": 0, \"sample_list\": []},\n",
    "    4: {\"counter\": 0, \"sample_list\": []},\n",
    "    5: {\"counter\": 0, \"sample_list\": []},\n",
    "    6: {\"counter\": 0, \"sample_list\": []},\n",
    "    7: {\"counter\": 0, \"sample_list\": []},\n",
    "    8: {\"counter\": 0, \"sample_list\": []},\n",
    "    9: {\"counter\": 0, \"sample_list\": []},\n",
    "    10: {\"counter\": 0, \"sample_list\": []},\n",
    "    11: {\"counter\": 0, \"sample_list\": []},\n",
    "    12: {\"counter\": 0, \"sample_list\": []},\n",
    "    13: {\"counter\": 0, \"sample_list\": []},\n",
    "    14: {\"counter\": 0, \"sample_list\": []},\n",
    "    15: {\"counter\": 0, \"sample_list\": []},\n",
    "    16: {\"counter\": 0, \"sample_list\": []},\n",
    "    17: {\"counter\": 0, \"sample_list\": []},\n",
    "    18: {\"counter\": 0, \"sample_list\": []},\n",
    "    19: {\"counter\": 0, \"sample_list\": []},\n",
    "    20: {\"counter\": 0, \"sample_list\": []},\n",
    "    21: {\"counter\": 0, \"sample_list\": []},\n",
    "    22: {\"counter\": 0, \"sample_list\": []},\n",
    "    23: {\"counter\": 0, \"sample_list\": []},\n",
    "    24: {\"counter\": 0, \"sample_list\": []},\n",
    "    25: {\"counter\": 0, \"sample_list\": []},\n",
    "    26: {\"counter\": 0, \"sample_list\": []}\n",
    "} \n",
    "        \n",
    "    \n",
    "\n",
    "class HSIGANDataset(Dataset):\n",
    "    def __init__(self, file_tracker, samples_per_img, img_size, std_threshold, x_bound, y_bound, transforms=None):\n",
    "        self.file_tracker = file_tracker \n",
    "        self.samples_per_img = samples_per_img\n",
    "        self.img_size = img_size\n",
    "        self.x_bound = x_bound\n",
    "        self.y_bound = y_bound\n",
    "        self.file_tracker_len = len(self.file_tracker)\n",
    "        self.total_len =  self.file_tracker_len * samples_per_img\n",
    "        self.std_threshold = std_threshold\n",
    "        self.samples_dict = samples_dict_init()    \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_to_sample = (idx % self.file_tracker_len) + 1\n",
    "        counter = self.samples_dict[img_to_sample][\"counter\"]\n",
    "        if (len(self.samples_dict[img_to_sample][\"sample_list\"]) == 0):\n",
    "            file_path = self.file_tracker[img_to_sample][\"path\"]\n",
    "            self.samples_dict[img_to_sample][\"sample_list\"] = get_list_img_samples(file_path, self.samples_per_img, self.img_size, self.x_bound, self.y_bound, self.std_threshold)\n",
    "        \n",
    "        sample = torch.tensor(self.samples_dict[img_to_sample][\"sample_list\"][counter])\n",
    "        #sample = sample.permute(2,0,1).contiguous()\n",
    "        self.samples_dict[img_to_sample][\"counter\"] = (counter + 1 ) % self.samples_per_img\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "\n",
    "\n",
    "def evaluate_and_save(epoch, dim, generator, real_samples, fid_metric, \n",
    "                      num_plot_samples=4, num_fid_samples=2000, fid_batch_size=32,\n",
    "                      output_dir=\"training_images_FID_244\", log_file=\"fid_scores.csv\"):    \n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    generator.eval()\n",
    "    device = next(generator.parameters()).device    \n",
    "   \n",
    "    d = real_samples.shape[0]\n",
    "    initial_sample_num = random.randint(0, max(0, d - num_plot_samples)) \n",
    "    final_sample_num = initial_sample_num + num_plot_samples\n",
    "    \n",
    "    real_to_plot = real_samples[initial_sample_num:final_sample_num].detach().cpu().numpy().transpose(0, 2, 3, 1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        noise = torch.randn(num_plot_samples, dim).to(device)\n",
    "        fake_to_plot = generator(noise).detach().cpu().numpy().transpose(0, 2, 3, 1)\n",
    "\n",
    "    print(f'Epoch {epoch+1} - Plotting and Calculating FID...')\n",
    "    fig, axes = plt.subplots(2, num_plot_samples, figsize=(12, 8))\n",
    "    fig.suptitle(f'Epoch {epoch+1}', fontsize=16)\n",
    "\n",
    "    if num_plot_samples == 1:\n",
    "        axes = np.expand_dims(axes, axis=1)\n",
    "\n",
    "    for i in range(num_plot_samples):\n",
    "        ax_real = axes[0, i]\n",
    "        ax_fake = axes[1, i]\n",
    "        \n",
    "        ax_real.imshow(np.clip(real_to_plot[i], 0, 1))\n",
    "        ax_real.set_title(\"Real\")\n",
    "        ax_real.axis('off')\n",
    "\n",
    "        ax_fake.imshow(np.clip(fake_to_plot[i], 0, 1))\n",
    "        ax_fake.set_title(\"Fake\")\n",
    "        ax_fake.axis('off')\n",
    "\n",
    "    filename = os.path.join(output_dir, f\"epoch_{epoch+1:03d}.png\")\n",
    "    fig.savefig(filename)\n",
    "    plt.close(fig)\n",
    "    \n",
    "    fid_metric.reset()\n",
    "    \n",
    "    actual_fid_samples = min(num_fid_samples, real_samples.shape[0])\n",
    "    num_batches = math.ceil(actual_fid_samples / fid_batch_size)\n",
    "\n",
    "    print(f\"Calculating FID on {actual_fid_samples} samples (Batch size: {fid_batch_size})...\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_batches):\n",
    "            start_idx = i * fid_batch_size\n",
    "            end_idx = min((i + 1) * fid_batch_size, actual_fid_samples)\n",
    "            \n",
    "            real_batch = real_samples[start_idx:end_idx].to(device)\n",
    "            \n",
    "            real_batch_uint8 = (real_batch * 255).byte()\n",
    "            \n",
    "            fid_metric.update(real_batch_uint8, real=True)\n",
    "            \n",
    "            current_batch_size = end_idx - start_idx \n",
    "            \n",
    "            noise_batch = torch.randn(current_batch_size, dim).to(device)\n",
    "            fake_batch = generator(noise_batch)\n",
    "            \n",
    "            fake_batch_uint8 = (fake_batch * 255).clamp(0, 255).byte()\n",
    "            \n",
    "            fid_metric.update(fake_batch_uint8, real=False)\n",
    "\n",
    "    fid_score = fid_metric.compute().item()\n",
    "    print(f\"Epoch {epoch+1} FID Score: {fid_score:.4f}\")\n",
    "\n",
    "    log_path = os.path.join(output_dir, log_file)\n",
    "    file_exists = os.path.isfile(log_path)\n",
    "    \n",
    "    with open(log_path, mode='a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        if not file_exists:\n",
    "            writer.writerow(['Epoch', 'FID'])\n",
    "        writer.writerow([epoch + 1, fid_score])\n",
    "\n",
    "    generator.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(epoch, dim, generator, real_samples, num_samples=4, output_dir=\"training_images_FID_244\"):\n",
    "   \n",
    "    d, _, _, _ = real_samples.shape\n",
    "    initial_sample_num = random.randint(0, d -  num_samples) \n",
    "    final_sample_num = initial_sample_num + num_samples\n",
    "    generator.eval()\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    real_to_plot = real_samples[initial_sample_num:final_sample_num, :, :, :].cpu().numpy().transpose(0, 2, 3, 1)\n",
    "    print(f'Epoch {epoch+1} - Real vs. Generated Samples')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        noise = torch.randn(num_samples, dim).to(device)\n",
    "        fake_to_plot = generator(noise)[:num_samples, :, :, :].cpu().numpy().transpose(0, 2, 3, 1)\n",
    "    \n",
    "\n",
    "    print(f\"Fake shape: {fake_to_plot.shape}\")\n",
    "    fig, axes = plt.subplots(2, num_samples, figsize=(12, 8))\n",
    "    fig.suptitle(f'Epoch {epoch+1}', fontsize=16)   \n",
    "\n",
    "    for i in range(num_samples):\n",
    "        if num_samples > 1:\n",
    "            ax_real = axes[0, i]\n",
    "            ax_fake = axes[1, i]\n",
    "        else: \n",
    "            ax_real = axes[0]\n",
    "            ax_fake = axes[1]\n",
    "\n",
    "        \n",
    "        img_real = real_to_plot[i][:, :, :]\n",
    "        img_fake = fake_to_plot[i][:, :, :]\n",
    "        \n",
    "        ax_real.imshow(np.clip(img_real, 0, 1))\n",
    "        ax_real.set_title(\"Real\")\n",
    "        ax_real.axis('off') # Hide the empty \"graph\" axes\n",
    "\n",
    "        ax_fake.imshow(np.clip(img_fake, 0, 1))\n",
    "        ax_fake.set_title(\"Fake\")\n",
    "        ax_fake.axis('off')    \n",
    "   \n",
    "    \n",
    "    filename = os.path.join(output_dir, f\"epoch_{epoch+1:03d}.png\")\n",
    "    fig.savefig(filename)\n",
    "    plt.close(fig)\n",
    "    \n",
    "    generator.train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = HSIGANDataset(file_tracker=file_tracker, samples_per_img=SAMPLES_PER_IMAGE, img_size=IMG_SIZE, std_threshold=STD_THRESHOLD, x_bound=X_BOUND, y_bound=Y_BOUND)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperSpectralDiscriminator(nn.Module):\n",
    "    def __init__(self, channels, img_size, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(channels, 32, kernel_size=6, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=6, stride=2, padding=2, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=6, stride=2, padding=2, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=6, stride=2, padding=2, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Flatten()           \n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, channels, img_size, img_size)\n",
    "            encoder_output_dim = self.main(dummy_input).shape[1]\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Linear(encoder_output_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.main(x)\n",
    "        x = self.final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperSpectralGenerator(nn.Module):\n",
    "    def __init__(self, channels, img_size, z_dim=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.init_size = (img_size - 20) // 16      \n",
    "\n",
    "        self.fc = nn.Linear(z_dim, 256 * self.init_size * self.init_size)\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            nn.Unflatten(1, (256, self.init_size, self.init_size)),\n",
    "            \n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=6, stride=2, padding=2, output_padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=6, stride=2, padding=2, output_padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2, padding=2, output_padding=0),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),            \n",
    "            \n",
    "            nn.ConvTranspose2d(32, channels, kernel_size=10, stride=2, padding=0, output_padding=0),\n",
    "            nn.Sigmoid() \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.main(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.10/site-packages/spectral/io/spyfile.py:224: NaNValueWarning: Image data contains NaN values.\n",
      "  warnings.warn('Image data contains NaN values.', NaNValueWarning)\n"
     ]
    }
   ],
   "source": [
    "real_label = 1\n",
    "fake_label = 0\n",
    "g_lr = 0.0002\n",
    "d_lr = 0.0002\n",
    "noise_size = Z_SIZE\n",
    "LOG_INTERVAL = 20    \n",
    "PLOT_INTERVAL = 100 #to test   \n",
    "\n",
    "netD = HyperSpectralDiscriminator(channels=CHANNELS, img_size=IMG_SIZE, num_classes=1).to(device)\n",
    "netD.apply(weights_init)\n",
    "netG = HyperSpectralGenerator(channels=CHANNELS, img_size=IMG_SIZE, z_dim=noise_size).to(device)\n",
    "netG.apply(weights_init)\n",
    "\n",
    "fixed_noise = torch.randn(64, noise_size, device=device)\n",
    "\n",
    "NUM_FID_SAMPLES = 2048\n",
    "all_real_samples = []\n",
    "\n",
    "\n",
    "temp_loader = iter(dataloader) \n",
    "count = 0\n",
    "\n",
    "while count < NUM_FID_SAMPLES:\n",
    "    try:       \n",
    "        batch = next(temp_loader)\n",
    "        \n",
    "        if isinstance(batch, list) or isinstance(batch, tuple):\n",
    "            batch = batch[0]\n",
    "            \n",
    "        all_real_samples.append(batch)\n",
    "        count += batch.shape[0]\n",
    "        \n",
    "    except StopIteration:\n",
    "        temp_loader = iter(dataloader)\n",
    "\n",
    "real_samples_large = torch.cat(all_real_samples, dim=0)\n",
    "\n",
    "real_samples_large = real_samples_large[:NUM_FID_SAMPLES]\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=d_lr, betas=(0.5, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=g_lr, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "[Epoch 1/2000] [Batch 20/24] [D loss: 0.3499] [G loss: 4.6844] [D Acc: 100.00%]\n",
      "[Epoch 2/2000] [Batch 20/24] [D loss: 1.0296] [G loss: 5.3108] [D Acc: 51.56%]\n",
      "[Epoch 3/2000] [Batch 20/24] [D loss: 0.4012] [G loss: 5.1869] [D Acc: 99.22%]\n",
      "[Epoch 4/2000] [Batch 20/24] [D loss: 0.4375] [G loss: 3.2470] [D Acc: 99.22%]\n",
      "[Epoch 5/2000] [Batch 20/24] [D loss: 0.7624] [G loss: 3.0543] [D Acc: 67.19%]\n",
      "[Epoch 6/2000] [Batch 20/24] [D loss: 0.6227] [G loss: 4.1071] [D Acc: 100.00%]\n",
      "[Epoch 7/2000] [Batch 20/24] [D loss: 0.4792] [G loss: 4.6388] [D Acc: 90.62%]\n",
      "[Epoch 8/2000] [Batch 20/24] [D loss: 0.9260] [G loss: 0.3810] [D Acc: 60.16%]\n",
      "[Epoch 9/2000] [Batch 20/24] [D loss: 0.5044] [G loss: 4.1600] [D Acc: 99.22%]\n",
      "[Epoch 10/2000] [Batch 20/24] [D loss: 0.5260] [G loss: 5.8380] [D Acc: 100.00%]\n",
      "[Epoch 11/2000] [Batch 20/24] [D loss: 1.4065] [G loss: 5.9342] [D Acc: 64.84%]\n",
      "[Epoch 12/2000] [Batch 20/24] [D loss: 0.4772] [G loss: 3.0320] [D Acc: 100.00%]\n",
      "[Epoch 13/2000] [Batch 20/24] [D loss: 0.6291] [G loss: 2.4721] [D Acc: 76.56%]\n",
      "[Epoch 14/2000] [Batch 20/24] [D loss: 0.9209] [G loss: 2.0521] [D Acc: 70.31%]\n",
      "[Epoch 15/2000] [Batch 20/24] [D loss: 1.2079] [G loss: 4.6116] [D Acc: 80.47%]\n",
      "[Epoch 16/2000] [Batch 20/24] [D loss: 1.3019] [G loss: 6.0979] [D Acc: 77.34%]\n",
      "[Epoch 17/2000] [Batch 20/24] [D loss: 1.1348] [G loss: 2.5397] [D Acc: 85.94%]\n",
      "[Epoch 18/2000] [Batch 20/24] [D loss: 0.7690] [G loss: 3.0782] [D Acc: 96.09%]\n",
      "[Epoch 19/2000] [Batch 20/24] [D loss: 0.7113] [G loss: 2.0774] [D Acc: 78.12%]\n",
      "[Epoch 20/2000] [Batch 20/24] [D loss: 0.9010] [G loss: 2.2470] [D Acc: 76.56%]\n",
      "[Epoch 21/2000] [Batch 20/24] [D loss: 1.0300] [G loss: 4.4853] [D Acc: 85.16%]\n",
      "[Epoch 22/2000] [Batch 20/24] [D loss: 0.9381] [G loss: 1.5438] [D Acc: 87.50%]\n",
      "[Epoch 23/2000] [Batch 20/24] [D loss: 1.2862] [G loss: 1.6925] [D Acc: 76.56%]\n",
      "[Epoch 24/2000] [Batch 20/24] [D loss: 0.7091] [G loss: 2.1968] [D Acc: 88.28%]\n",
      "[Epoch 25/2000] [Batch 20/24] [D loss: 1.0499] [G loss: 1.0374] [D Acc: 57.81%]\n",
      "[Epoch 26/2000] [Batch 20/24] [D loss: 0.8316] [G loss: 0.9152] [D Acc: 58.59%]\n",
      "[Epoch 27/2000] [Batch 20/24] [D loss: 1.0634] [G loss: 3.9700] [D Acc: 90.62%]\n",
      "[Epoch 28/2000] [Batch 20/24] [D loss: 0.7313] [G loss: 1.6375] [D Acc: 67.97%]\n",
      "[Epoch 29/2000] [Batch 20/24] [D loss: 0.9435] [G loss: 0.7392] [D Acc: 59.38%]\n",
      "[Epoch 30/2000] [Batch 20/24] [D loss: 0.6612] [G loss: 2.0525] [D Acc: 89.84%]\n",
      "[Epoch 31/2000] [Batch 20/24] [D loss: 0.9691] [G loss: 1.3924] [D Acc: 56.25%]\n",
      "[Epoch 32/2000] [Batch 20/24] [D loss: 0.9319] [G loss: 1.8965] [D Acc: 71.88%]\n",
      "[Epoch 33/2000] [Batch 20/24] [D loss: 1.1164] [G loss: 1.3021] [D Acc: 51.56%]\n",
      "[Epoch 34/2000] [Batch 20/24] [D loss: 0.6636] [G loss: 2.5524] [D Acc: 82.03%]\n",
      "[Epoch 35/2000] [Batch 20/24] [D loss: 0.9681] [G loss: 2.1094] [D Acc: 89.84%]\n",
      "[Epoch 63/2000] [Batch 20/24] [D loss: 0.8666] [G loss: 1.8201] [D Acc: 63.28%]\n",
      "[Epoch 64/2000] [Batch 20/24] [D loss: 1.3196] [G loss: 3.2606] [D Acc: 75.78%]\n",
      "[Epoch 65/2000] [Batch 20/24] [D loss: 1.3001] [G loss: 2.1199] [D Acc: 77.34%]\n",
      "[Epoch 66/2000] [Batch 20/24] [D loss: 0.5209] [G loss: 2.7780] [D Acc: 91.41%]\n",
      "[Epoch 67/2000] [Batch 20/24] [D loss: 0.6506] [G loss: 2.8323] [D Acc: 73.44%]\n",
      "[Epoch 68/2000] [Batch 20/24] [D loss: 0.9198] [G loss: 2.3571] [D Acc: 81.25%]\n",
      "[Epoch 69/2000] [Batch 20/24] [D loss: 1.1388] [G loss: 1.1437] [D Acc: 53.12%]\n",
      "[Epoch 70/2000] [Batch 20/24] [D loss: 0.7085] [G loss: 2.4615] [D Acc: 72.66%]\n",
      "[Epoch 71/2000] [Batch 20/24] [D loss: 1.2061] [G loss: 2.4777] [D Acc: 81.25%]\n",
      "[Epoch 72/2000] [Batch 20/24] [D loss: 1.1955] [G loss: 1.5977] [D Acc: 54.69%]\n",
      "[Epoch 73/2000] [Batch 20/24] [D loss: 1.5305] [G loss: 3.7358] [D Acc: 67.19%]\n",
      "[Epoch 74/2000] [Batch 20/24] [D loss: 0.7055] [G loss: 1.6364] [D Acc: 83.59%]\n",
      "[Epoch 75/2000] [Batch 20/24] [D loss: 1.0406] [G loss: 2.3274] [D Acc: 93.75%]\n",
      "[Epoch 76/2000] [Batch 20/24] [D loss: 0.8616] [G loss: 2.4588] [D Acc: 68.75%]\n",
      "[Epoch 77/2000] [Batch 20/24] [D loss: 0.7475] [G loss: 2.7126] [D Acc: 77.34%]\n",
      "[Epoch 78/2000] [Batch 20/24] [D loss: 1.1621] [G loss: 1.8312] [D Acc: 57.03%]\n",
      "[Epoch 79/2000] [Batch 20/24] [D loss: 0.7483] [G loss: 1.9198] [D Acc: 64.84%]\n",
      "[Epoch 80/2000] [Batch 20/24] [D loss: 1.2123] [G loss: 1.1832] [D Acc: 60.94%]\n",
      "[Epoch 81/2000] [Batch 20/24] [D loss: 1.2458] [G loss: 2.2922] [D Acc: 75.00%]\n",
      "[Epoch 82/2000] [Batch 20/24] [D loss: 1.0476] [G loss: 2.2813] [D Acc: 85.94%]\n",
      "[Epoch 83/2000] [Batch 20/24] [D loss: 1.0296] [G loss: 2.3883] [D Acc: 74.22%]\n",
      "[Epoch 84/2000] [Batch 20/24] [D loss: 1.2078] [G loss: 0.8154] [D Acc: 53.91%]\n",
      "[Epoch 85/2000] [Batch 20/24] [D loss: 0.7390] [G loss: 2.2857] [D Acc: 80.47%]\n",
      "[Epoch 86/2000] [Batch 20/24] [D loss: 0.9475] [G loss: 1.1906] [D Acc: 55.47%]\n",
      "[Epoch 87/2000] [Batch 20/24] [D loss: 0.6828] [G loss: 2.9863] [D Acc: 75.00%]\n",
      "[Epoch 88/2000] [Batch 20/24] [D loss: 1.4122] [G loss: 2.7277] [D Acc: 71.09%]\n",
      "[Epoch 89/2000] [Batch 20/24] [D loss: 1.1777] [G loss: 1.3885] [D Acc: 60.94%]\n",
      "[Epoch 90/2000] [Batch 20/24] [D loss: 1.1852] [G loss: 1.1623] [D Acc: 57.03%]\n",
      "[Epoch 91/2000] [Batch 20/24] [D loss: 0.9021] [G loss: 1.3751] [D Acc: 57.03%]\n",
      "[Epoch 92/2000] [Batch 20/24] [D loss: 0.9047] [G loss: 1.8288] [D Acc: 79.69%]\n",
      "[Epoch 93/2000] [Batch 20/24] [D loss: 1.4545] [G loss: 0.7287] [D Acc: 51.56%]\n",
      "[Epoch 94/2000] [Batch 20/24] [D loss: 0.6823] [G loss: 4.2392] [D Acc: 98.44%]\n",
      "[Epoch 95/2000] [Batch 20/24] [D loss: 1.0821] [G loss: 2.4381] [D Acc: 89.84%]\n",
      "[Epoch 96/2000] [Batch 20/24] [D loss: 0.7715] [G loss: 2.9926] [D Acc: 97.66%]\n",
      "[Epoch 97/2000] [Batch 20/24] [D loss: 1.0429] [G loss: 1.6272] [D Acc: 50.78%]\n",
      "[Epoch 98/2000] [Batch 20/24] [D loss: 0.7845] [G loss: 1.9811] [D Acc: 65.62%]\n",
      "[Epoch 99/2000] [Batch 20/24] [D loss: 1.1404] [G loss: 1.0134] [D Acc: 56.25%]\n",
      "[Epoch 100/2000] [Batch 20/24] [D loss: 0.8433] [G loss: 3.7455] [D Acc: 92.19%]\n",
      "--- Generating plot for epoch 100 ---\n",
      "Epoch 100 - Plotting and Calculating FID...\n",
      "Calculating FID on 1000 samples (Batch size: 32)...\n",
      "Epoch 100 FID Score: 204.2586\n",
      "[Epoch 101/2000] [Batch 20/24] [D loss: 1.0428] [G loss: 3.2964] [D Acc: 90.62%]\n",
      "[Epoch 102/2000] [Batch 20/24] [D loss: 1.1467] [G loss: 2.1512] [D Acc: 66.41%]\n",
      "[Epoch 103/2000] [Batch 20/24] [D loss: 0.8092] [G loss: 1.8257] [D Acc: 98.44%]\n",
      "[Epoch 104/2000] [Batch 20/24] [D loss: 0.7324] [G loss: 2.5072] [D Acc: 82.03%]\n",
      "[Epoch 105/2000] [Batch 20/24] [D loss: 1.0759] [G loss: 0.9143] [D Acc: 57.03%]\n",
      "[Epoch 106/2000] [Batch 20/24] [D loss: 0.7283] [G loss: 3.8441] [D Acc: 98.44%]\n",
      "[Epoch 107/2000] [Batch 20/24] [D loss: 0.6448] [G loss: 3.3242] [D Acc: 97.66%]\n",
      "[Epoch 108/2000] [Batch 20/24] [D loss: 0.3965] [G loss: 3.3827] [D Acc: 99.22%]\n",
      "[Epoch 109/2000] [Batch 20/24] [D loss: 1.4453] [G loss: 4.8494] [D Acc: 71.09%]\n",
      "[Epoch 110/2000] [Batch 20/24] [D loss: 0.6252] [G loss: 2.2524] [D Acc: 77.34%]\n",
      "[Epoch 111/2000] [Batch 20/24] [D loss: 1.0329] [G loss: 2.3814] [D Acc: 71.88%]\n",
      "[Epoch 112/2000] [Batch 20/24] [D loss: 0.6689] [G loss: 3.7179] [D Acc: 80.47%]\n",
      "[Epoch 113/2000] [Batch 20/24] [D loss: 0.4489] [G loss: 3.3187] [D Acc: 98.44%]\n",
      "[Epoch 114/2000] [Batch 20/24] [D loss: 1.1606] [G loss: 1.4311] [D Acc: 51.56%]\n",
      "[Epoch 115/2000] [Batch 20/24] [D loss: 2.1810] [G loss: 0.7689] [D Acc: 50.00%]\n",
      "[Epoch 116/2000] [Batch 20/24] [D loss: 0.6834] [G loss: 4.8641] [D Acc: 98.44%]\n",
      "[Epoch 117/2000] [Batch 20/24] [D loss: 0.4601] [G loss: 3.9199] [D Acc: 100.00%]\n",
      "[Epoch 118/2000] [Batch 20/24] [D loss: 0.5491] [G loss: 2.5261] [D Acc: 92.19%]\n",
      "[Epoch 119/2000] [Batch 20/24] [D loss: 0.4352] [G loss: 4.4232] [D Acc: 98.44%]\n",
      "[Epoch 120/2000] [Batch 20/24] [D loss: 1.0388] [G loss: 0.8240] [D Acc: 63.28%]\n",
      "[Epoch 121/2000] [Batch 20/24] [D loss: 1.1847] [G loss: 3.9232] [D Acc: 79.69%]\n",
      "[Epoch 122/2000] [Batch 20/24] [D loss: 0.5038] [G loss: 3.6142] [D Acc: 90.62%]\n",
      "[Epoch 123/2000] [Batch 20/24] [D loss: 0.5105] [G loss: 3.9246] [D Acc: 98.44%]\n",
      "[Epoch 124/2000] [Batch 20/24] [D loss: 1.0422] [G loss: 2.2462] [D Acc: 75.78%]\n",
      "[Epoch 125/2000] [Batch 20/24] [D loss: 0.8616] [G loss: 1.2111] [D Acc: 65.62%]\n",
      "[Epoch 126/2000] [Batch 20/24] [D loss: 0.5413] [G loss: 3.0402] [D Acc: 92.19%]\n",
      "[Epoch 127/2000] [Batch 20/24] [D loss: 1.3187] [G loss: 0.8894] [D Acc: 50.78%]\n",
      "[Epoch 128/2000] [Batch 20/24] [D loss: 0.6222] [G loss: 3.6374] [D Acc: 99.22%]\n",
      "[Epoch 129/2000] [Batch 20/24] [D loss: 0.6620] [G loss: 5.0470] [D Acc: 75.78%]\n",
      "[Epoch 130/2000] [Batch 20/24] [D loss: 1.0632] [G loss: 3.8131] [D Acc: 81.25%]\n",
      "[Epoch 131/2000] [Batch 20/24] [D loss: 1.1072] [G loss: 0.9592] [D Acc: 59.38%]\n",
      "[Epoch 132/2000] [Batch 20/24] [D loss: 1.1651] [G loss: 4.8846] [D Acc: 82.03%]\n",
      "[Epoch 133/2000] [Batch 20/24] [D loss: 0.6369] [G loss: 4.1788] [D Acc: 98.44%]\n",
      "[Epoch 134/2000] [Batch 20/24] [D loss: 0.5114] [G loss: 2.9883] [D Acc: 85.94%]\n",
      "[Epoch 135/2000] [Batch 20/24] [D loss: 0.7707] [G loss: 1.5730] [D Acc: 75.00%]\n",
      "[Epoch 136/2000] [Batch 20/24] [D loss: 1.2111] [G loss: 4.4643] [D Acc: 80.47%]\n",
      "[Epoch 137/2000] [Batch 20/24] [D loss: 0.6364] [G loss: 3.3639] [D Acc: 99.22%]\n",
      "[Epoch 138/2000] [Batch 20/24] [D loss: 0.6039] [G loss: 4.9170] [D Acc: 97.66%]\n",
      "[Epoch 139/2000] [Batch 20/24] [D loss: 0.4180] [G loss: 4.6871] [D Acc: 95.31%]\n",
      "[Epoch 140/2000] [Batch 20/24] [D loss: 1.2404] [G loss: 2.4006] [D Acc: 79.69%]\n",
      "[Epoch 141/2000] [Batch 20/24] [D loss: 1.2359] [G loss: 2.0569] [D Acc: 61.72%]\n",
      "[Epoch 142/2000] [Batch 20/24] [D loss: 0.4177] [G loss: 4.3471] [D Acc: 96.88%]\n",
      "[Epoch 143/2000] [Batch 20/24] [D loss: 0.9274] [G loss: 5.3957] [D Acc: 57.81%]\n",
      "[Epoch 144/2000] [Batch 20/24] [D loss: 0.5177] [G loss: 3.8164] [D Acc: 85.16%]\n",
      "[Epoch 145/2000] [Batch 20/24] [D loss: 0.9279] [G loss: 3.3554] [D Acc: 76.56%]\n",
      "[Epoch 146/2000] [Batch 20/24] [D loss: 0.4062] [G loss: 4.8967] [D Acc: 96.88%]\n",
      "[Epoch 147/2000] [Batch 20/24] [D loss: 0.6663] [G loss: 5.5557] [D Acc: 73.44%]\n",
      "[Epoch 148/2000] [Batch 20/24] [D loss: 0.5171] [G loss: 3.6893] [D Acc: 100.00%]\n",
      "[Epoch 149/2000] [Batch 20/24] [D loss: 0.5849] [G loss: 4.5769] [D Acc: 98.44%]\n",
      "[Epoch 150/2000] [Batch 20/24] [D loss: 0.4181] [G loss: 4.2839] [D Acc: 94.53%]\n",
      "[Epoch 475/2000] [Batch 20/24] [D loss: 0.5806] [G loss: 3.8923] [D Acc: 80.47%]\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "D_accuracies = []\n",
    "iters = 0\n",
    "num_epochs =  2000 \n",
    "\n",
    "fid = FrechetInceptionDistance(feature=2048).to(device)\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    netD.train()\n",
    "    for i, samples in enumerate(dataloader):        \n",
    "        samples = samples.to(device)        \n",
    "        batch_size = samples.size(0)\n",
    "        real_labels_D = torch.full((batch_size, 1), 0.9, device=device)\n",
    "        real_labels_G = torch.ones(batch_size, 1).to(device)\n",
    "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "        optimizerD.zero_grad()\n",
    "\n",
    "        real_output = netD(samples) \n",
    "\n",
    "        d_loss_real = criterion(real_output, real_labels_D)\n",
    "        real_acc = ((real_output > 0.5).float() == real_labels_G).float().mean()\n",
    "       \n",
    "        noise = torch.randn(batch_size, noise_size, device=device)\n",
    "        fake_spectra = netG(noise).squeeze()\n",
    "        \n",
    "        fake_output = netD(fake_spectra.detach())        \n",
    "        d_loss_fake = criterion(fake_output, fake_labels)\n",
    "\n",
    "        fake_acc = (fake_output < 0.5).float().mean()\n",
    "\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_loss.backward()\n",
    "        \n",
    "        optimizerD.step()\n",
    "\n",
    "        d_accuracy = (real_acc + fake_acc) / 2\n",
    "\n",
    "        optimizerG.zero_grad()\n",
    "        fake_spectra_for_g = netG(noise).squeeze()\n",
    "        output = netD(fake_spectra_for_g)\n",
    "\n",
    "        g_loss = criterion(output, real_labels_G)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "        if (i + 1) % LOG_INTERVAL == 0:\n",
    "            print(\n",
    "                f\"[Epoch {epoch+1}/{num_epochs}] [Batch {i+1}/{len(dataloader)}] \"\n",
    "                f\"[D loss: {d_loss.item():.4f}] [G loss: {g_loss.item():.4f}] \"\n",
    "                f\"[D Acc: {d_accuracy.item():.2%}]\"\n",
    "            )\n",
    "            D_losses.append(d_loss.item())\n",
    "            G_losses.append(g_loss.item())\n",
    "            D_accuracies.append(d_accuracy.item())\n",
    "        \n",
    "    if (epoch + 1) % PLOT_INTERVAL == 0:\n",
    "        print(f\"--- Generating plot for epoch {epoch+1} ---\")\n",
    "        #plot_samples(epoch=epoch, generator=netG, real_samples=fixed_real_samples,dim=noise_size)\n",
    "        evaluate_and_save(epoch=epoch,dim=noise_size,generator=netG,real_samples=real_samples_large,fid_metric=fid, num_plot_samples=4, num_fid_samples=1000 # Use at least 1000 for decent FID estimation\n",
    ")\n",
    "\n",
    "\n",
    "print(\"--- Training Finished ---\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
